{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGzo9LPelzaD8+bzqnq98W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Josephoduro/mlWithColab/blob/main/trivialHW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gJOgps5oC5O",
        "outputId": "683b14dc-ff7b-4b2a-c12d-8c49c3618bd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "number_of_datapoints = 100\n",
        "\n",
        "#generate random x values in the range -5 to +5\n",
        "x = np.random.uniform(low = -5, high = 5, size = (number_of_datapoints, 1))\n",
        "\n",
        "\n",
        "\n",
        "#generate random x values in the range -5 to +5\n",
        "y = np.random.uniform(low = -5, high = 5, size = (number_of_datapoints, 1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#we set up the relation between x and y using a linear equation:\n",
        "\n",
        "z = 7 * x + 6 * y + 5\n",
        "\n",
        "#generate the noise using random function as earlier with the values ranging from\n",
        "#-1 to +1 with the statement below\n",
        "\n",
        "noise = np.random.uniform(low = -1 , high = 1, size =(number_of_datapoints, 1))\n",
        "\n",
        "#Now create the z array using the linear equation and adding noise to it as shown\n",
        "#in the statement below:\n",
        "\n",
        "z = 7 * x + 6 * y + 5 + noise\n",
        "\n",
        "#It is to be noted that the input to our neural network is a single\n",
        "#dimentional array consisting of 100 rows with each row consisting of\n",
        "#another single dimansional array having x and y values as columns.\n",
        "#To create the required input data format, you use the cilumn_stack function\n",
        "#as shown below:\n",
        "\n",
        "input =  np.column_stack((x,y))\n",
        "\n",
        "\n",
        "\n",
        "#Defining the neural network module\n",
        "\n",
        "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape =[2])])\n",
        "\n",
        "#the units parameter defines the dimensionality of the output space\n",
        "\n",
        "#COMPILING MODEL\n",
        "#To train a model, we define a learning process. The components of learning\n",
        "#process: Objective loss function, optimizer and metrics.\n",
        "\n",
        "#1. Set up the learning process by calling the compile function on the model.\n",
        "#The compile takes  the three parameters as its arguments.\n",
        "#SDG = stochastic gradient descent, MSE= Mean Squared Error\n",
        "\n",
        "model.compile(optimizer = 'sgd',\n",
        "              loss = 'mean_squared_error',\n",
        "              metrics = ['mse'])\n",
        "\n",
        "#Training Network\n",
        "#The model is trained in several iterations by assigning preset weights to\n",
        "#various nodes in the network. This repeated by adjusting the weight to minimize\n",
        "#errors. This process is called epochs\n",
        "#To save this state we create a history object in keras\n",
        "\n",
        "from keras.callbacks import History\n",
        "history = History()\n",
        "\n",
        "#To begin training\n",
        "\n",
        "model.fit(input, z, epochs = 15 , verbose = 1, validation_split = 0.2,\n",
        "          callbacks = [history])\n",
        "\n",
        "#First parameter species our stacked input\n",
        "#Second parameter specify target values\n",
        "#Epochs defines number of iterations\n",
        "#Verbose specify if we want to specify the training process\n",
        "#Validation_split parameter specifies that 20% of the given data would be used\n",
        "#for validating the trained model\n",
        "#Callbacks specifies where intermediate monitoring data would be stored\n",
        "\n",
        "\n",
        "#Examining training outputs\n",
        "#We can examine what is recorded in history variable by:\n",
        "print(history.history.keys())\n",
        "\n",
        "#To create a loss of both mse and loss, we print the loss on both the training\n",
        "#validation data, use the following:\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Accuracy')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc = 'upper right')\n",
        "plt.show()\n",
        "\n",
        "#it will be observed from the plot that our model is fully trained after the\n",
        "#the 15th epoch and the error is greatly minimized after the third epoch\n",
        "\n",
        "#To further verify this claim we will print the MSE to further verify this claim.\n",
        "#To plot the MSE, we use:\n",
        "\n",
        "plt.plot(history.history['mse'])\n",
        "plt.plot(history.history['val_mse'])\n",
        "plt.title('mean squared error')\n",
        "plt.ylabel('mse')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend(['train', 'validation'] , loc = 'upper right')\n",
        "plt.show\n",
        "\n",
        "\n",
        "#We can further verify by plot the predicted output vs. the real output\n",
        "#using the following code segment:\n",
        "\n",
        "plt.plot(np.squeeze(model.predict_on_batch(input)),\n",
        "         np.squeeze(z))\n",
        "plt.xlabel('predicted output')\n",
        "plt.ylabel('real output')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6wu7mHAbrFzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting\n",
        "#To make a prediction on unseen x and y values, you will use the predict\n",
        "#function on the trained model. This is shown in the following:\n",
        "\n",
        "print(\"Predicted z for x = 2, y =3 -----> \", model.predict([[2,3]]).round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFtSqVKXai7f",
        "outputId": "c9e7f909-c193-427e-d6b2-f9edcfc58e61"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 64ms/step\n",
            "Predicted z for x = 2, y =3 ----->  [[35.55]]\n"
          ]
        }
      ]
    }
  ]
}